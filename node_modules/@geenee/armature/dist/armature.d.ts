/** Size */
interface Size {
    /** Width */
    width: number;
    /** Height */
    height: number;
}
/** Image input types (supported) */
declare type ImageInput = ImageData | ImageBytes | HTMLCanvasElement;
/** Image bytes buffer */
interface ImageBytes {
    /** Pixel buffer */
    data: Uint8Array;
    /** Width in pixels */
    width: number;
    /** Height in pixels */
    height: number;
}

declare type Args<F> = [F] extends [(...args: infer U) => any] ? U : [F] extends [void] ? [] : [F];
/**
 * EventEmitter class
 *
 * These objects expose an on() function that allows one or more
 * functions to be attached to named events emitted by the object.
 * When the EventEmitter object emits an event, all of the functions
 * attached to that specific event are called synchronously. Any
 * values returned by the called listeners are ignored and discarded.
 *
 * @typeParam Events - Events emitted by EventEmitter
 */
interface EventEmitterTI<Events> {
    /**
     * Adds the listener function to the event
     *
     * @param event - The name of the event
     * @param listener - The callback function
     * @returns This EventEmitter
     */
    on<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Adds a one-time listener function for the event
     *
     * @param event - The name of the event
     * @param listener - The callback function
     * @returns This EventEmitter
     */
    once<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Removes the listener from the listener array for the event
     *
     * @param event - The name of the event
     * @param listener - The callback function
     * @returns This EventEmitter
     */
    off<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Adds the listener function to the end of the listeners array
     *
     * @param event - The name of the event
     * @param listener - The callback function
     * @returns This EventEmitter
     */
    addListener<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Adds the listener function to the beginning of the listeners array
     *
     * @param event - The name of the event
     * @param listener - The callback function
     * @returns This EventEmitter
     */
    prependListener<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Adds a one-time listener function to the beginning of the listeners array
     *
     * @param event - The name of the event
     * @param listener - The callback function
     */
    prependOnceListener<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Removes the specified listener from the listener array
     *
     * @param event - The name of the event
     * @param listener - The callback function
     * @returns This EventEmitter
     */
    removeListener<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Removes all listeners, or those of the specified event
     *
     * @param event - The name of the event
     * @returns This EventEmitter
     */
    removeAllListeners<E extends keyof Events>(event?: E): this;
    /**
     * Synchronously calls each of the listeners registered for the event
     *
     * @param event - The name of the event
     * @param args - Arguments passed to the listeners
     * @returns True if the event had listeners, False otherwise
     */
    emit<E extends keyof Events>(event: E, ...args: Args<Events[E]>): boolean;
    /**
     * The number of listeners listening to the event
     *
     * @param event - The name of the event
     * @returns Number of listeners
     */
    listenerCount<E extends keyof Events>(event: E): number;
    /**
     * Copy of the array of listeners for the event
     *
     * @param event - The name of the event
     * @returns Copy of the listeners array
     */
    listeners<E extends keyof Events>(event: E): Function[];
    /**
     * Copy of the array of listeners for the event including wrappers
     *
     * @param event The name of the event
     * @returns Copy of the listeners array
     */
    rawListeners<E extends keyof Events>(event: E): Function[];
    /**
     * Sets maximum number of listeners per event
     *
     * @param n - Maximum number of listeners
     * @returns This EventEmitter
     */
    setMaxListeners(n: number): this;
    /**
     * Maximum number of listeners per event
     * @returns Maximum number of listeners per event
     */
    getMaxListeners(): number;
    /**
     * List of emitter's events
     * @returns List of emitter's events
     */
    eventNames(): (keyof Events | string | symbol)[];
}
declare const EventEmitterT_base: new <Events_1>() => EventEmitterTI<Events_1>;
/**
 * EventEmitter generic class
 *
 * These objects expose an on() function that allows one or more
 * functions to be attached to named events emitted by the object.
 * When the EventEmitter object emits an event, all of the functions
 * attached to that specific event are called synchronously. Any
 * values returned by the called listeners are ignored and discarded.
 *
 * @typeParam Events - Events emitted by EventEmitter
 */
declare class EventEmitterT<Events> extends EventEmitterT_base<Events> {
}

/**
 * Image buffer
 *
 * Helper class grabbing images into internal storage.
 * Used by {@link VideoSource} to grab video streams and
 * {@link Engine} to resize images for faster processing.
 * Image can be accessed by canvas or read as pixels.
 */
declare class ImageBuffer {
    /** Size of video */
    size: Size;
    /** Canvas to capture frames */
    canvas: HTMLCanvasElement;
    /** Context to capture frames */
    protected context: CanvasRenderingContext2D | null;
    /**
     * Constructor
     *
     * @param name - Optional name of canvas
     */
    constructor(name?: string);
    /**
     * Grab frame
     *
     * Grabs frame from video element or canvas into internal canvas.
     * Automatically rescales an image if resolutions are different.
     *
     * @param video - Element to grab image from
     * @returns True if image is grabbed, False otherwise
     */
    capture(video: HTMLVideoElement | HTMLCanvasElement): boolean;
    /**
     * Get image buffer
     *
     * Returns data buffer of image currently grabbed into canvas.
     *
     * @returns Image data buffer on success, undefined otherwise
     */
    data(): ImageData | undefined;
    /** Set video size */
    setSize(size: Size): void;
    /** Fill canvas with color */
    fill(): void;
    /** Dispose video context object */
    dispose(): void;
}
/** Parameters of video capture */
interface VideoParams {
    /** Size requested from a camera */
    size?: Size;
    /** Request rear-facing camera if presented */
    rear?: boolean;
}
/** Setup parameters of video capture */
declare type VideoSourceParams = VideoParams | MediaStreamConstraints | MediaStream | string;
/** Events emitted by {@link VideoCapture} */
interface CaptureEvents {
    /** Video resize */
    resize: (size: Size) => void;
}
/**
 * Video source
 *
 * General class of video capture objects providing
 * functionality to grab images from various sources.
 * It implements basic interfaces like setup, start,
 * stop of the video capture, and grabbing of frames.
 * Internally utilizes {@link ImageBuffer} as storages.
 */
declare class VideoSource extends EventEmitterT<CaptureEvents> {
    /** Context of original stream */
    buffer: ImageBuffer;
    /** Timestamp of the last captured frame */
    captureTime: number;
    /** Timer to emulate timestamps */
    private timer?;
    /** Constructor */
    constructor();
    /**
     * Setup video source
     *
     * Sets up capture, overridden for particular video source.
     * Video source can be setup by simplified {@link VideoParams}
     * opening default front/rear camera with provided resolution,
     * custom MediaStreamConstraints providing the most flexible
     * way to setup video stream (e.g. set deviceId), or external
     * MediaStream allowing custom video sources (e.g. from file).
     * Default implementation sets video resolution according to
     * size in {@link VideoParams} or 1920x1080 as the fallback.
     * It captures static image, canvas is filled by white color.
     *
     * @param params - Parameters of video capture
     * @returns Promise resolved to the status of setup when done
     * @virtual
     */
    setup(params?: VideoSourceParams): Promise<boolean>;
    /**
     * Dispose video source object
     *
     * @virtual
     */
    dispose(): void;
    /**
     * Start video capture
     *
     * Video capture can be started only after successful setup().
     *
     * @returns Promise resolved when capture is started
     * @virtual
     */
    start(): Promise<void>;
    /**
     * Pause video capture
     *
     * @virtual
     */
    pause(): void;
    /**
     * Reset video capture
     *
     * After reset() capture may be started again only after setup().
     *
     * @virtual
     */
    reset(): void;
    /**
     * Grab the next video frame
     *
     * VideoSource grabs the static image permanently
     * stored in embedded {@link ImageBuffer} object.
     *
     * @returns True if next frame was available and grabbed
     * @virtual
     */
    capture(): boolean;
    /**
     * Resolution of the video stream
     *
     * @returns Resolution of the video stream
     */
    size(): Size;
    /**
     * Aspect ratio of the video stream
     *
     * @returns Aspect ratio of the video stream
     */
    ratio(): number;
    /**
     * Update callback on video resize
     *
     * @param size - New size of the video stream
     */
    protected updateSize(size: Size): void;
}

/** Basic processor parameters */
interface ProcParams {
    /**
     * The SDK access token. Mandatory parameter authenticating
     * your user account and providing access to the SDK on the
     * current url. You can create tokens for required urls on
     * your [account page](https://builder.geenee.ar/sdk). Token
     * must be provided to initialize {@link Engine#init | Engine}.
     * [More](https://lab.geen.ee/engeenee-doc/#getting-started).
     */
    token: string;
    /**
     * Root path to computational modules (statically served wasms).
     * The SDK requires access to wasm modules provided within its
     * packages. By default, the root path is the current url "./".
     * Root can be set on {@link Engine#init | Engine initialization}.
     */
    root?: string;
    /** Cache computational modules (experimental) */
    cache?: boolean;
}
/** Events emitted by {@link Processor} */
interface ProcessorEvents {
    /** Processor initialized */
    init: (status: boolean) => void;
    /** Processor reset */
    reset: () => void;
}
/**
 * Core generic processor
 *
 * Processor is a computational core of any application and the
 * most essential part of the {@link Engine} in an app's pipeline.
 * Processing results are used by {@link Renderer} to update scene.
 * Every processor must define methods to initialize and release
 * instances required for image processing, and evaluation of
 * processing results on provided image (where all logic happens).
 * Processor is a generic abstract class defining common API.
 *
 * @typeParam ResultT - Type of processing results
 * @typeParam ParamsT - Type of processor parameters
 */
declare class Processor<ResultT extends {} = {}, ParamsT extends ProcParams = ProcParams> extends EventEmitterT<ProcessorEvents> {
    /** Processor parameters */
    protected params: Partial<ParamsT>;
    /** Resolution of input video */
    protected videoSize: Size;
    /** Aspect ratio of input video */
    protected videoRatio: number;
    /** Recommended maximum size of input */
    optimalSize: number;
    /** Camera aspect ratio */
    cameraRatio: number;
    /** Camera vertical angle in radians */
    cameraAngle: number;
    /**
     * Constructor
     *
     * @param params - Processor parameters
     */
    constructor();
    /**
     * Process the image
     *
     * Main method defining the logic of video processing.
     * Overridden by derived classes for particular application.
     *
     * @param input - Image
     * @param timestamp - Timestamp
     * @virtual
     */
    process(input: ImageInput, timestamp?: number): Promise<ResultT | undefined>;
    /**
     * Initialize processor
     *
     * Initializes all resources required for video processing.
     * Overridden by derived classes for particular application.
     *
     * @param params - Processor parameters
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @returns Status of initialization
     * @virtual
     */
    init(params: ParamsT, size?: Size, ratio?: number): Promise<boolean>;
    /**
     * Reset processor
     *
     * Resets all processing instances to the initial state.
     * Overridden by derived classes for particular processing.
     *
     * @virtual
     */
    reset(): void;
    /**
     * Dispose processor object
     *
     * Releases resources and instances allocated by processor.
     * Processor object cannot be used after calling dispose().
     * Overridden by derived classes for particular processing.
     *
     * @virtual
     */
    dispose(): void;
    /**
     * Set resolution of the input video
     *
     * Could be overridden to adjust processing pipeline.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @virtual
     */
    setupVideo(size: Size, ratio?: number): void;
}

/** Events emitted by {@link Renderer} */
interface RendererEvents {
    /** Renderer initialized */
    load: () => void;
    /** Scene updated, emitted on every iteration */
    render: () => void;
    /** Rendering canvas resized */
    resize: (size: Size, ratio: number) => void;
}
/**
 * Core generic renderer
 *
 * Renderer is the core visualization part of any application.
 * It's attached to {@link Engine}. Results of processing and
 * captured frame are provided to renderer that updates scene,
 * visualization and application logic according to this data.
 * Basically, renders define two methods load() and update().
 * The first one is used to initialize all assets and prepare
 * the scene e.g. set up lightning, environment map. Engine
 * will call load() method during pipeline initialization.
 * The second one is used to update the scene using results
 * of video processing. This's where all the logic happens.
 * By overriding/extending {@link Renderer#load | load()} and
 * {@link Renderer#update | update()} you can add any custom
 * logic, interactions, animations, post-processing effects,
 * gesture recognition, physics, etc. to an app. Renderer is
 * a generic abstract class defining common core interfaces.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class Renderer<ResultT> extends EventEmitterT<RendererEvents> {
    /** Loaded state */
    protected loaded: boolean;
    /** Resolution of input video */
    protected videoSize: Size;
    /** Aspect ratio of input video */
    protected videoRatio: number;
    /** Camera aspect ratio */
    protected cameraRatio: number;
    /** Camera vertical angle in radians */
    protected cameraAngle: number;
    /**
     * Constructor
     */
    constructor();
    /**
     * Initialize renderer
     *
     * Initializes renderer, all required assets, and the scene.
     * Overridden by derived classes for particular application.
     *
     * @returns Promise resolving when initialization is finished
     * @virtual
     */
    load(): Promise<void>;
    /**
     * Reset renderer
     *
     * Releases all resources and instances created in load().
     * Overridden by derived classes for particular application.
     *
     * @virtual
     */
    unload(): void;
    /**
     * Update the scene
     *
     * Main method defining the logic of the renderer.
     * Updates the scene according to provided results.
     * Overridden by derived classes for the application.
     *
     * @param result - Results of video processing
     * @param stream - Captured video frame
     * @returns Promise resolving when update is finished
     * @virtual
     */
    update(result: ResultT, stream: HTMLCanvasElement): Promise<void>;
    /**
     * Update the video layer
     *
     * Virtual method drawing input video frame.
     * Derived renderer provides implementation.
     *
     * @param stream - Captured video frame
     * @virtual
     */
    protected updateVideo(stream: HTMLCanvasElement): void;
    /**
     * Update and render the scene
     *
     * Virtual method updating and rendering the scene.
     * Overridden by implementation of derived renderer.
     *
     * @virtual
     */
    protected updateScene(): void;
    /**
     * Dispose renderer object
     *
     * @virtual
     */
    dispose(): void;
    /**
     * Set video parameters
     *
     * Could be overridden to adjust rendering pipeline.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @virtual
     */
    setupVideo(size: Size, ratio?: number): void;
    /**
     * Set camera parameters
     *
     * Some video processors statically define camera parameters
     * from just a video resolution. In this cases setupCamera()
     * is used to pass these static camera parameters to renderer.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @virtual
     */
    setupCamera(ratio: number, angle: number): void;
}

/** Parameters of Engine */
interface EngineParams {
    /** Maximum processing size (resizing). If not defined
     * engine uses image size preferred by processor. */
    max?: number;
    /** Preserve original resolution, default - true. */
    orig?: boolean;
}
/** Events emitted by {@link Engine} */
interface EngineEvents {
    /** Engine initialized */
    init: (status: boolean) => void;
    /** Engine set up */
    setup: (status: boolean) => void;
    /** Pipeline started */
    start: () => void;
    /** Pipeline stopped */
    pause: () => void;
}
/**
 * Core generic engine
 *
 * Engine is a core of any app and organizer of a pipeline.
 * It's responsible to interact with lower-level instances and
 * at the same time provide simple and user-friendly interface.
 * Engine combines together data (video) capturing, processing
 * and rendering. It's created for particular {@link Processor}.
 * Processor's constructor is provided to the engine and former
 * initializes, sets up and controls state of processor during
 * life-circle of the application. Results of processing and
 * captured image are passed to a {@link Renderer} attached to
 * the Engine. Notable feature is fast rescaling of images for
 * processing down to the requested resolution in cases when
 * available camera output is bigger. Engine parameters have an
 * option to limit processed image size. Original video stream
 * can be preserved on request, this is useful when processing
 * cannot handle high resolution images but you still want to
 * render high quality video in your app. All core components:
 * Engine, Processor, and Renderer are generics parametrized by
 * type of processing results and optionally tuning parameters.
 *
 * @typeParam ProcessorT - Type of processor
 * @typeParam ResultT - Type of processing results
 * @typeParam ParamsT - Type of processor parameters
 */
declare class Engine<ResultT extends {}, ParamsT extends ProcParams, ProcessorT extends Processor<ResultT, ParamsT>> extends EventEmitterT<EngineEvents> {
    protected engineParams?: EngineParams | undefined;
    /** Processor utilized by the engine */
    protected processor: ProcessorT;
    /** Renderer attached to the engine */
    protected renderers: Renderer<ResultT>[];
    /** Video source instance */
    protected video: VideoSource;
    /** Ratio of video stream */
    protected videoRatio: number;
    /** Shallow copy of canvas with video for renderers */
    protected streamCanvas?: HTMLCanvasElement;
    /** Size of video for renderers */
    protected streamSize: Size;
    /** Shallow copy of canvas with video for processors */
    protected processCanvas?: HTMLCanvasElement;
    /** Size of video for processors */
    protected processSize: Size;
    /** Buffer to resize frames */
    protected resizeBuffer?: ImageBuffer;
    /** Original stream is resized */
    protected resizeEnabled: boolean;
    /** State of the pipeline */
    private loopState;
    /** Id of current frame */
    private loopId?;
    /**
     * Constructor
     *
     * @param Processor - Processor class or its constructor
     * @param engineParams - Parameters of the engine
     * @param Source - Video source class or its constructor
     * @typeParam ProcessorT - Type of processor
     * @typeParam ResultT - Type of processing results
     * @typeParam ParamsT - Type of processor parameters
     */
    constructor(Processor: new () => ProcessorT, engineParams?: EngineParams | undefined, Source?: new () => VideoSource);
    /**
     * Initialize engine. Sets up processor.
     *
     * The SDK {@link ProcParams#token | access token} is
     * required parameter that authenticates the user and
     * enables the SDK on the current url. By default, path
     * to required wasm modules provided with SDK packages
     * is the current url. You can change the root path to
     * wasms passing {@link ProcParams#root | root parameter}.
     *
     * @param - Parameters of the processor
     * @returns Status of initialization
     */
    init: (procParams: ParamsT) => Promise<boolean>;
    /**
     * Setup engine. Initializes video capture.
     *
     * Video capture can be setup by simplified {@link VideoParams}
     * opening default front/rear camera with provided resolution,
     * custom MediaStreamConstraints providing the most flexible
     * way to setup video stream (e.g. set deviceId), or external
     * MediaStream allowing custom video sources (e.g. from file).
     *
     * @param videoParams - Parameters of video capture
     * @returns Status of initialization
     */
    setup: (videoParams?: VideoSourceParams) => Promise<boolean>;
    /**
     * Start pipeline.
     *
     * Pipeline can be started only after successful init and setup.
     */
    start: () => Promise<void>;
    /**
     * Pause pipeline.
     *
     * Nothing happens if pipeline is not started yet.
     */
    pause: () => void;
    /**
     * Reset pipeline
     *
     * Stops pipeline, resets video capture and processor.
     * After reset one needs to reinitialize video capture
     * calling setup() before pipeline can be started again.
     */
    reset: () => void;
    /**
     * Attach Renderer to the engine
     *
     * @param renderer - Object to be attached
     */
    addRenderer(renderer: Renderer<ResultT>): Promise<void>;
    /**
     * Remove attached Renderer
     *
     * @param renderer - Renderer to be removed
     */
    removeRenderer(renderer: Renderer<ResultT>): void;
    /** Iterate */
    protected iterate: () => Promise<void>;
    /** Enqueue the next iteration */
    protected enqueue(): void;
    /** Setup processor */
    protected setupProcessor(procParams: ParamsT): Promise<boolean>;
    /** Setup video capture */
    protected setupVideo(videoParams?: VideoSourceParams): Promise<boolean>;
    /** Setup video size */
    protected setupSize(size: Size): Promise<void>;
    /**
     * Callback called when video resolution is changed
     *
     * @param size - Size of the video
     */
    protected resizeVideo(size: Size): void;
}

/**
 * Generic asynchronous engine
 *
 * AsyncEngine is extension of the {@link Engine} that does
 * processing in the background. Its pipeline provides for
 * better performance and more stable frames per second. By
 * using async engine you can achieve smoother and faster
 * experience. AsyncEngine and Engine are compatible, you
 * can use any of them without additional code adjustments.
 *
 * @typeParam ProcessorT - Type of processor
 * @typeParam ResultT - Type of processing results
 * @typeParam ParamsT - Type of processor parameters
 */
declare class AsyncEngine<ResultT extends {}, ParamsT extends ProcParams, ProcessorT extends Processor<ResultT, ParamsT>> extends Engine<ResultT, ParamsT, ProcessorT> {
    /** Promise with result of async processing */
    private result?;
    /** Iterate
     *
     * Overridden for asynchronous processing.
    */
    protected iterate: () => Promise<void>;
}

/**
 * Video capture
 *
 * VideoCapture object provides means to grab images from a source.
 * It implements all required functionality including video device
 * initialization, setup, start/stop of capture and frame grabbing.
 * Internally VideoCapture utilizes {@link ImageBuffer} as storage.
 */
declare class VideoCapture extends VideoSource {
    /** Video element */
    protected videoRef: HTMLVideoElement;
    /** Time shift for video loop */
    private timeShift;
    /** Constructor */
    constructor();
    /**
     * Setup video capture
     *
     * Sets up video device, streams and contexts with canvases.
     * Video capture can be setup by simplified {@link VideoParams}
     * opening default front/rear camera with provided resolution,
     * custom MediaStreamConstraints providing the most flexible
     * way to select the video stream (for example set deviceId),
     * external MediaStream allowing use of custom video sources,
     * or string defining a media file as source of video stream.
     *
     * @param params - Parameters of video capture
     * @returns Promise resolved to the status of setup when finished
     */
    setup(params?: VideoSourceParams): Promise<boolean>;
    /**
     * Dispose video capture object
     *
     * @override
     */
    dispose(): void;
    /**
     * Start video capture
     *
     * Video capture can be started only after successful setup().
     *
     * @returns Promise resolved when capture is started
     */
    start(): Promise<void>;
    /**
     * Pause video capture
     *
     * @override
     */
    pause(): void;
    /**
     * Reset video capture
     *
     * After reset() capture may be started again only after setup().
     *
     * @override
     */
    reset(): void;
    /**
     * Grab the next video frame
     *
     * VideoCapture grabs the next image from a video source
     * (camera) and stores it in embedded {@link ImageBuffer}.
     *
     * @returns True if next frame was available and grabbed
     * @override
     */
    capture(): boolean;
}

/** Events emitted by {@link ResponsiveCanvas} */
interface CanvasEvents {
    /** Resize event */
    resize: () => void;
}
/** {@link ResponsiveCanvas} fitting modes */
declare type CanvasMode = "crop" | "fit" | "pad";
/**
 * Responsive canvas
 *
 * Responsive canvas is useful helper for applications
 * doing rendering. It creates canvas layers within any
 * html element. Layers preserve aspect ratio according
 * to fitting mode. Ratio of canvas usually follows the
 * ratio of the input video. In "crop" mode canvases are
 * cropped to have the same aspect ratio as the container
 * and scaled to fill the whole area. This is default
 * mode preferred in most cases. In "fit mode" canvases
 * are scaled down to fit into the container and leaving
 * margins to stay in the center, "pad" mode behavior is
 * the same, but margins are filled with highly blurred
 * parts of the input video instead of still background.
 * ResponsiveCanvas tracks size changes of container and
 * updates layers and margins to preserve aspect ratio.
 */
declare class ResponsiveCanvas extends EventEmitterT<CanvasEvents> {
    protected container: HTMLElement;
    protected mode: CanvasMode;
    protected layerCount: number;
    protected mirror: boolean;
    protected aspectRatio: number;
    /** Canvas layers */
    layers: HTMLCanvasElement[];
    /** Padding canvases */
    pads?: [HTMLCanvasElement, HTMLCanvasElement];
    /** Relative size of padding canvases */
    padsSize: [number, number];
    /** Resize observer */
    protected observer?: ResizeObserver;
    /**
     * Constructor
     *
     * @param container - Container of responsive canvas
     * @param mode - Fitting mode
     * @param layerCount - Number of canvas layers
     * @param mirror - Mirror the output
     * @param aspectRatio - Target aspect ratio
     */
    constructor(container: HTMLElement, mode?: CanvasMode, layerCount?: number, mirror?: boolean, aspectRatio?: number);
    /**
     * Set aspect ratio
     *
     * Responsive canvas will preserve provided aspect ratio and
     * resize itself within container according to fitting mode.
     *
     * @param ratio - Aspect ratio
     */
    setAspectRatio: (ratio: number) => void;
    /**
     * Set mirror mode
     *
     * ResponsiveCanvas mirrors the output layers.
     *
     * @param ratio - Mirror the output
     */
    setMirror: (mirror: boolean) => void;
    /**
     * Update canvas sizes to preserve aspect ratio
     *
     * Updates relative sizes of canvas within container to
     * preserve fixed aspect ratio according to fitting mode.
     * Called when container is resized (by resize observer).
     *
     * @param containerRatio - Aspect ratio of the container
     */
    protected updateSizes: (containerRatio: number) => void;
    /**
     * Resize event callback
     *
     * Callback attached to resize observer to handle size updates.
     * Basically, it calls updateSizes() to adjust canvas sizes.
     *
     * @param entries - Elements being resized
     */
    protected handleResize: (entries: ResizeObserverEntry[]) => void;
}

/**
 * Generic renderer plugin
 *
 * Plugin can be attached to an instances of {@link Renderer}.
 * Usually they perform simple tasks that can be separated from
 * bigger app context into atomic building blocks, for example
 * control object on a scene to follow (be attached to) user's
 * head, apply image effect (smoothing, beautification), recognize
 * gestures or poses, notify about state changes or perform other
 * kinds of transformations, pre/post-processing, or analyzes with
 * a 3D scene, video stream or raw data from a {@link Processor}.
 * Plugin is a abstraction level to single out ready-made helpers
 * that can be reused as atomic building blocks of an application.
 * Plugins are very similar to Renderer and also should implement
 * two basic methods load() and update(). Renderer initializes a
 * plugin calling load() and providing itself as argument, plugin
 * in turn remembers renderer it's attached to and gets access to
 * required resources of the renderer, for example 2d or webgl
 * context of a canvas or reference to a 3d scene. In update()
 * method plugins implement actual logic given processing results.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class Plugin<ResultT extends {} = {}> {
    /** Renderer loaded the plugin */
    protected renderer?: Renderer<ResultT>;
    /** Loaded state */
    loaded: boolean;
    /**
     * Initialize plugin
     *
     * Initializes resources/instances needed by plugin.
     * Overridden by derived classes for particular task.
     *
     * @param renderer - Renderer this plugin is attached to
     * @returns Promise resolving when initialization is finished
     * @virtual
     */
    load(renderer: Renderer<ResultT>): Promise<void>;
    /**
     * Reset plugin
     *
     * Releases all resources/instances created in load().
     * Overridden by derived classes for particular task.
     *
     * @virtual
     */
    unload(): void;
    /**
     * Update
     *
     * Main method implementing the logic of the plugin.
     * Overridden by derived classes for particular task.
     *
     * @param result - Results of video processing
     * @param stream - Captured video frame
     * @returns Promise resolving when update is finished
     * @virtual
     */
    update(result: ResultT, stream: HTMLCanvasElement): Promise<void>;
    /** Dispose video plugin */
    dispose(): void;
    /**
     * Set video size
     *
     * Could be overridden to adjust plugin's pipeline.
     *
     * @param size - Resolution of input video
     * @virtual
     */
    setupVideo(size: Size): void;
}

/**
 * Generic plugin renderer
 *
 * Extends {@link Renderer} implementing render plugin system.
 * Plugins can be attached to an instance of the PluginRenderer.
 * Usually they perform simple tasks that can be separated from
 * bigger app context into atomic building blocks, for example
 * control object on a scene to follow (be attached to) user's
 * head, apply image effect (smoothing, beautification), recognize
 * gestures or poses, notify about state changes or perform other
 * kinds of transformations, pre/post-processing, or analyzes with
 * a 3D scene, video stream or raw data from a {@link Processor}.
 * {@link Plugin} is a abstraction level to single out ready-made
 * helpers that can be reused as atomic building blocks of an app.
 * Plugins are very similar to Renderer but do only one task, they
 * also should implement two basic methods load() and update().
 * PluginRenderer initializes all attached plugins calling their
 * load() method and providing itself as an argument for plugin
 * to acquire required resources, for example canvas context or
 * reference to 3d scene. Every rendering cycle PluginRenderer
 * calls update() of all attached and successfully loaded plugins
 * passing results of video processing and current video frame.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class PluginRenderer<ResultT extends {} = {}> extends Renderer<ResultT> {
    /** Attached plugins */
    protected plugins: Plugin<ResultT>[];
    /**
     * Initialize renderer
     *
     * Initializes all attached plugins.
     *
     * @returns Promise resolving when initialization is finished
     * @override
     */
    load(): Promise<void>;
    /**
     * Reset renderer
     *
     * Resets all attached plugins.
     *
     * @override
     */
    unload(): void;
    /**
     * Update the scene
     *
     * Updates all attached plugins.
     *
     * @param result - Results of video processing
     * @param stream - Captured video frame
     * @returns Promise resolving when update is finished
     * @override
     */
    update(result: ResultT, stream: HTMLCanvasElement): Promise<void>;
    /**
     * Update attached plugins
     *
     * Calls update() of all attached plugins.
     *
     * @param result - Results of video processing
     * @param stream - Captured video frame
     * @returns Promise resolving when update is finished
     */
    protected updatePlugins(result: ResultT, stream: HTMLCanvasElement): Promise<void>;
    /**
     * Dispose renderer object
     *
     * Extended to dispose all attached plugins.
     *
     * @override
     */
    dispose(): void;
    /**
     * Add render plugin
     *
     * Initializes the plugin if it's not loaded yet but renderer is ready.
     * Renderer takes ownership of the plugin instance meaning it will
     * release it when plugin is detached or renderer is disposed itself.
     */
    addPlugin(plugin: Plugin<ResultT>): Promise<void>;
    /**
     * Remove render plugin
     *
     * Renderer will dispose the plugin before detaching it.
     */
    removePlugin(plugin: Plugin<ResultT>): void;
    /**
     * Remove all render plugins
     *
     * Renderer will dispose all plugins before detaching them.
     */
    removeAllPlugins(): void;
    /**
     * Set video parameters
     *
     * Callback sets up all attached plugins.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
}

/** Parameters of {@link CanvasRenderer} */
interface CanvasParams {
    /** Container of responsive canvas */
    container: HTMLElement;
    /** Fitting mode */
    mode?: CanvasMode;
    /** Number of canvas layers */
    layerCount?: number;
    /** Mirror the output */
    mirror?: boolean;
    /** Target aspect ratio */
    aspectRatio?: number;
}
/**
 * Generic renderer using {@link ResponsiveCanvas}
 *
 * Generic {@link Renderer} utilizing {@link ResponsiveCanvas}.
 * Refer to their documentation for more details. CanvasRenderer
 * can have several layers and there're two basic usage patterns.
 * Use separate layers for video and scene and effectively render
 * scene on top of the video stream. Advantage of this approach
 * is that image and scene can be processed independently and one
 * can apply different effects or postprocessing. This pattern is
 * also easier to implement. Or one can use only one canvas layer
 * and embed video stream into the scene as object via a texture
 * or background component. This approach will have more complex
 * implementation dependent on particular renderer. On the other
 * hand, rendering effects affecting the whole scene will also
 * apply to the video stream. This can improve performance and
 * allows advanced rendering/postprocessing techniques to be used.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class CanvasRenderer<ResultT extends {} = {}> extends PluginRenderer<ResultT> {
    /** Responsive canvas */
    canvas: ResponsiveCanvas;
    /** Drawing context of padding canvases */
    protected padCtx: [
        CanvasRenderingContext2D | null,
        CanvasRenderingContext2D | null
    ];
    /**
     * Constructor
     *
     * @param params - Parameters of responsive canvas
     */
    constructor(params: CanvasParams);
    /**
     * Update the video layer
     *
     * Updates padding canvases using portions of video frame.
     *
     * @param stream - Captured video frame
     * @override
     */
    protected updateVideo(stream: HTMLCanvasElement): void;
    /**
     * Set video parameters
     *
     * Callback passes aspect ratio to responsive canvas.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
    /**
     * Set mirror mode
     *
     * CanvasRenderer sets mirror mode of ResponsiveCanvas.
     *
     * @param ratio - Mirror the output
     */
    setMirror(mirror: boolean): void;
    /**
     * Update padding canvases
     *
     * Updates padding canvases using portions of video frame.
     *
     * @param stream - Captured video frame
     */
    protected updatePads(stream: HTMLCanvasElement): void;
    /**
     * Setup padding canvases
     *
     * Callback sets up size of padding canvases.
     */
    protected setupPadding: () => void;
}

/**
 * Generic video renderer
 *
 * VideoRenderer is based on {@link CanvasRenderer} and uses
 * two canvas layers: one for video stream and another to
 * render 3D scene on top of it. This usage pattern is the
 * easiest to implement, but more limited as video is not
 * embedded into the scene and e.g. renderer's postprocess
 * effects or advanced techniques can't be applied to video.
 * It's possible to draw on top of the video stream using
 * 2d canvas context, for example to add simple 2d effects.
 * Effects can be encapsulated in a form of render plugins.
 * Plugins are levels of abstraction allowing to single out
 * ready-made helpers that are used as atomic building blocks.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class VideoRenderer<ResultT extends {} = {}> extends CanvasRenderer<ResultT> {
    /** Drawing context of video canvas layer */
    videoCtx: CanvasRenderingContext2D | null;
    /**
     * Constructor
     *
     * @param params - Parameters of responsive canvas
     */
    constructor(params: CanvasParams);
    /**
     * Update the video layer
     *
     * Draws input video frame on corresponding canvas layer.
     *
     * @param stream - Captured video frame
     * @override
     */
    protected updateVideo(stream: HTMLCanvasElement): void;
    /**
     * Set video parameters
     *
     * Callback sets up size of video canvas layer.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
}

/** Image source accepted by {@link ImageTexture} */
declare type ImageSource = ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement | Uint8Array;
/**
 * Image texture
 *
 * Helper class storing image as a WebGL texture.
 * Used by shader classes and renderers as wrapper
 * around images providing convenient interfaces.
 */
declare class ImageTexture {
    protected gl: WebGL2RenderingContext;
    protected size: Size;
    protected grayscale: boolean;
    protected linear: boolean;
    /** Input image texture */
    protected buffer: WebGLTexture | null;
    /**
     * Constructor
     *
     * @param gl - Context where texture is allocated
     * @param size - Size of the image
     */
    constructor(gl: WebGL2RenderingContext, size?: Size, grayscale?: boolean, linear?: boolean);
    /**
     * Update texture
     *
     * @param image - Image to upload into texture
     */
    update(image: ImageSource): void;
    /**
     * Resize texture
     *
     * @param size - New size of the image
     * @returns True when successfully resized, false otherwise
     */
    resize(size: Size): boolean;
    /**
     * Dispose allocated resources
     */
    dispose(): void;
    /**
     * Texture instance
     *
     * @returns Instance of the texture
     */
    texture(): WebGLTexture | null;
    /**
     * Whether texture is allocated and valid
     *
     * @returns True if texture is allocated, false otherwise
     */
    valid(): boolean;
}

/** Uniform types */
declare type UniformType = "1f" | "2f" | "3f" | "4f";
/**
 * Generic image processing shader program
 *
 * Helper class encapsulating all instances of a simple
 * image processing shaders, including vertex & fragment
 * shaders, output texture, frame buffer, and uniforms.
 * {@link ShaderProgram#process} method applies shaders
 * to input textures, results are written to the output
 * texture, {@link ShaderProgram#render} method renders
 * directly on the canvas element providing WebGL context.
 * Simplest program processes input texture covering it
 * it by 2 triangles and passing texture coordinates to
 * a fragment shader where all image processing happens.
 */
declare class ShaderProgram {
    protected gl: WebGL2RenderingContext;
    protected size: Size;
    protected inputs: string[];
    protected uniforms: {
        [key: string]: UniformType;
    };
    /** Vertex shader */
    protected vertShader: WebGLShader | null;
    /** Fragment shader */
    protected fragShader: WebGLShader | null;
    /** Shader program */
    protected shaderProgram: WebGLProgram | null;
    /** Vertex buffer */
    protected vertBuffer: WebGLBuffer | null;
    /** Frame buffer (output) */
    protected frameBuffer: WebGLFramebuffer | null;
    /** Unifrom locations */
    protected uniformsLoc: {
        [key: string]: WebGLUniformLocation | null;
    };
    /** Position attribute location */
    protected positionLoc: number;
    /** Output image texture */
    output?: ImageTexture;
    /**
     * Constructor
     *
     * @param gl - WebGL context where program is instantiated
     * @param size - Size of processed (input & output) image
     * @param inputs - Shader texture inputs (names of sampler uniforms)
     * @param uniforms - Shader uniforms as name-type map object
     * @param fragSrc - Fragment shader source (copy shader by default)
     * @param vertSrc - Vertex shader source (copy shader by default)
     * @param outputLinear - Texture read interpolation mode
     */
    constructor(gl: WebGL2RenderingContext, size?: Size, inputs?: string[], uniforms?: {
        [key: string]: UniformType;
    }, fragSrc?: string, vertSrc?: string, outputLinear?: boolean);
    /**
     * Process input image
     *
     * Applies shader program to input image, output
     * is written to image texture using framebuffer.
     *
     * @param inputs - Input image textures
     * @param uniforms - Values of shader uniforms
     */
    process(inputs: (WebGLTexture | null)[], uniforms?: {
        [key: string]: number[];
    }): void;
    /**
     * Process input image and render the result
     *
     * Applies shader program to input image, output
     * is renderer to canvas providing WebGL context.
     *
     * @param inputs - Input image textures
     * @param uniforms - Values of shader uniforms
     */
    render(inputs: (WebGLTexture | null)[], uniforms?: {
        [key: string]: number[];
    }): void;
    /**
     * Resize
     *
     * Resizes output image texture and updates uniforms.
     *
     * @param size - Main/output shader size
     */
    resize(size: Size): void;
    /**
     * Dispose program object
     *
     * Releases resources and instances allocated by program.
     * Program object cannot be used after calling dispose().
     */
    dispose(): void;
    /**
     * Shader program instance
     *
     * @returns Instance of shader program
     */
    program(): WebGLProgram | null;
    /**
     * Compile shader program
     *
     * Allocates and sets up all resources required
     * for shader program, compiles and links shaders.
     *
     * @param fragSrc - Fragment shader source (copy shader by default)
     * @param vertSrc - Vertex shader source (copy shader by default)
     */
    protected compile(fragSrc: string, vertSrc: string): void;
    /**
     * Prepare execution of the shader
     *
     * Set input textures and provided uniform values,
     * bind program and setup vertex attribute arrays.
     *
     * @param gl - Context of shader program
     * @param inputs - Input image textures
     * @param uniforms - Values of shader uniforms
     */
    protected prepare(gl: WebGL2RenderingContext, inputs: (WebGLTexture | null)[], uniforms?: {
        [key: string]: number[];
    }): void;
    /**
     * Save context state
     *
     * Saves state of WebGL including program in use,
     * bound array buffers, framebuffer, and textures.
     * Returned state can be restored later to safely
     * share context with other frameworks or renderers.
     *
     * @param gl - Context which state will be recorded
     * @returns Current state of WebGL context
     */
    protected save(gl: WebGL2RenderingContext): {
        program: WebGLProgram | null;
        arrayBuffer: WebGLBuffer | null;
        framebuffer: WebGLFramebuffer | null;
        vertexArray: WebGLVertexArrayObject | null;
        viewport: Int32Array;
        scissor: Int32Array;
        cullFace: boolean;
        activeTexture: number;
        textures: (WebGLTexture | null)[];
    };
    /**
     * Restore context state
     *
     * Restores state of WebGL context to recorded checkpoint.
     * This allows to safely share context with other engines.
     *
     * @param gl - Context to restore state
     * @param state - Previously recorded state
     */
    protected restore(gl: WebGL2RenderingContext, state: ReturnType<ShaderProgram["save"]>): void;
}

/**
 * Generic shader renderer
 *
 * ShaderRenderer is based on {@link CanvasRenderer} and uses
 * two canvas layers: one for video stream and another to
 * render 3D scene on top of it. Video is rendered by WebGL
 * shaders, this allows to apply complex computationally
 * demanding post-processing effects to the input stream.
 * For example, simple mono-chrome or sepia effects, or more
 * complex face beatification and dynamic geometry filters.
 * Shader effects can be encapsulated in a form of plugins.
 * Plugins are levels of abstraction allowing to single out
 * ready-made helpers that are used as atomic building blocks.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class ShaderRenderer<ResultT extends {} = {}> extends CanvasRenderer<ResultT> {
    /** Context of the video canvas layer */
    shaderCtx: WebGL2RenderingContext | null;
    /** Rendering shader */
    protected shader?: ShaderProgram;
    /** Input image texture */
    protected input?: ImageTexture;
    /** Current image texture */
    current: WebGLTexture | null;
    /**
     * Constructor
     *
     * @param params - Parameters of responsive canvas
     */
    constructor(params: CanvasParams);
    /**
     * Initialize renderer
     *
     * Initializes rendering context, shader program and buffers.
     *
     * @returns Promise resolving when initialization is finished
     * @override
     */
    load(): Promise<void>;
    /**
     * Reset renderer
     *
     * Releases all resources and instances created in load().
     * Releases rendering context, shader program and buffers.
     *
     * @override
     */
    unload(): void;
    /**
     * Update the scene
     *
     * Renderers input video frame on corresponding canvas layer.
     *
     * @param result - Results of video processing
     * @param stream - Captured video frame
     * @returns Promise resolving when update is finished
     * @override
     */
    update(result: ResultT, stream: HTMLCanvasElement): Promise<void>;
    /**
     * Update the video layer
     *
     * Calls rendering shaders with current input texture.
     *
     * @param stream - Captured video frame
     * @override
     */
    protected updateVideo(stream: HTMLCanvasElement): void;
    /**
     * Set video parameters
     *
     * Callback sets up size of video canvas layer,
     * resizes texture and allocates new storage.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
}

/**
 * Generic scene renderer
 *
 * Extends {@link VideoRenderer} to be used with the particular
 * WebGL engine e.g. Babylon.js or Three.js. Type of the scene
 * is additional parametrization of generic. {@link ScenePlugin}
 * written for WebGL engine can be attached to a SceneRenderer.
 * Usually they perform simple scene tasks that can be separated
 * from the main context into atomic building blocks, for example
 * control node of a scene to follow (be attached to) user's head
 * or replace its geometry with detected face mesh (mask effect).
 *
 * @typeParam ResultT - Type of processing results
 * @typeParam SceneT - Type of renderer's scene
 */
declare class SceneRenderer<ResultT extends {} = {}, SceneT = undefined> extends ShaderRenderer<ResultT> {
    /** Renderer scene */
    scene?: SceneT;
    /**
     * Dispose renderer object
     *
     * Extended to dispose scene object.
     *
     * @override
     */
    dispose(): void;
}

/**
 * Generic video plugin
 *
 * VideoPlugin is a specialization of a {@link Plugin} for
 * {@link VideoRenderer}. Usually they perform simple 2D
 * drawing tasks on a video canvas (for example, simplest
 * face effects or adding debug information / graphics).
 * VideoPlugin gets access to 2d canvas of VideoRenderer
 * in load() and draws on this canvas directly in update(),
 * on top of video frame using provided processing data.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class VideoPlugin<ResultT extends {} = {}> extends Plugin<ResultT> {
    /** Drawing context of video canvas layer */
    protected videoCtx?: CanvasRenderingContext2D;
    /**
     * Initialize plugin
     *
     * Initializes everything required for image drawing.
     * Acquires video canvas 2d context of the renderer.
     *
     * @param renderer - Renderer this plugin is attached to
     * @returns Promise resolving when initialization is finished
     * @override
     */
    load(renderer: Renderer<ResultT>): Promise<void>;
    /**
     * Reset plugin
     *
     * Releases all resources allocated in load().
     * Deletes the reference to 2d canvas context.
     *
     * @override
     */
    unload(): void;
}

/**
 * Generic shader plugin
 *
 * ShaderPlugin is a specialization of a {@link Plugin} for
 * {@link ShaderRenderer}. They apply complex computationally
 * demanding post-processing effects to the input stream.
 * For example, simple mono-chrome or sepia effects, or more
 * complex face beatification and dynamic geometry filters.
 * ShaderPlugin shares webgl context with the main renderer.
 * Basic implementation uses {@link ShaderProgram} created
 * for shaders provided to plugin's constructor. Plugins are
 * organized in chain within ShaderRenderer, input of the
 * next shader is output of the previous and initial input is
 * original video image, output of the last plugin is rendered.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class ShaderPlugin<ResultT extends {} = {}> extends Plugin<ResultT> {
    protected inputs?: string[] | undefined;
    protected uniforms?: {
        [key: string]: UniformType;
    } | undefined;
    protected fragSrc?: string | undefined;
    protected vertSrc?: string | undefined;
    /** Rendering shader */
    protected shader?: ShaderProgram;
    /** Image size */
    protected size: Size;
    /**
     * Constructor
     *
     * @param inputs - Shader texture inputs (names of sampler uniforms)
     * @param uniforms - Shader uniforms as name-type map object
     * @param fragSrc - Code of fragment shader (copy shader by default)
     * @param vertSrc - Vertex shader source (copy shader by default)
     */
    constructor(inputs?: string[] | undefined, uniforms?: {
        [key: string]: UniformType;
    } | undefined, fragSrc?: string | undefined, vertSrc?: string | undefined);
    /**
     * Initialize plugin
     *
     * Initializes resources required for shader effect.
     * Acquires webgl context of the main webgl renderer.
     * Basic implementation creates and compiles rendering
     * program for shaders provided to plugin's constructor.
     * Overridden by derived classes for particular effect.
     *
     * @param renderer - Renderer this plugin is attached to
     * @returns Promise resolving when initialization is finished
     * @override
     */
    load(renderer: Renderer<ResultT>): Promise<void>;
    /**
     * Reset plugin
     *
     * Releases all resources and instances created in load().
     * Overridden by derived classes for particular effect.
     *
     * @override
     */
    unload(): void;
    /**
     * Update the image
     *
     * Main method implementing webgl shader effect or filter.
     * {@link ShaderRenderer} has current image texture that
     * will be rendered. ShaderPlugin uses current texture as
     * input and writes results to {@link ShaderPlugin#output}.
     * {@link ShaderPlugin#output} becomes new current texture
     * of ShaderRenderer. This way all ShaderPlugins attached
     * to renderer organize a chain of effects applied on top
     * of each other. Method process() implements shader effect
     * itself, it's intended to be overridden by effect authors.
     *
     * @param result - Results of video processing
     * @param stream - Captured video frame
     * @returns Promise resolving when update is finished
     * @override @sealed
     */
    update(result: ResultT, stream: HTMLCanvasElement): Promise<void>;
    /**
     * Process the image
     *
     * Main method implementing webgl shader effect or filter.
     * It's called by update() and should be overridden to add
     * custom logic. process() should return true if effect is
     * successfully applied and false if it is skipped, for
     * example, if effect applies only when result is not empty.
     *
     * @param result - Results of video processing
     * @param input - Current image texture
     * @returns True on success, false otherwise
     * @virtual
     */
    process(result: ResultT, input: WebGLTexture): boolean;
    /**
     * Set video size
     *
     * Adjusts shader and texture to a new size.
     *
     * @override
     */
    setupVideo(size: Size): void;
}

/**
 * Generic scene plugin
 *
 * ScenePlugins can be attached to SceneRenderer instances.
 * Usually they control a scene node and implement simple
 * tasks that can be separated from main rendering context.
 * For example, make a scene node follow (be attached to)
 * person's head, or make node an occluder, or create face
 * mesh node and set texture as a mask. On load() plugin
 * prepares or modifies the attached node if required and
 * reference to the scene object is cached to be used in
 * update() and unload(); update() implements main logic
 * and updates the scene node according to provided results.
 *
 * @typeParam ResultT - Type of processing results
 * @typeParam SceneT - Type of renderer's scene
 */
declare class ScenePlugin<ResultT extends {} = {}, SceneT = undefined> extends Plugin<ResultT> {
    /** Reference to a scene instance */
    protected scene?: SceneT;
    /**
     * Initialize plugin
     *
     * Prepares or modifies the attached node if required.
     * Reference to the scene object is cached and used by
     * plugin on update() and unload(). You need to reload
     * plugin if you want to change scene it's attached to.
     *
     * @param renderer - Renderer this plugin is attached to
     * @returns Promise resolving when initialization is finished
     * @override
     */
    load(renderer: Renderer<ResultT>): Promise<void>;
    /**
     * Reset plugin
     *
     * Releases all resources allocated in load().
     * Deletes cached reference to the scene object.
     *
     * @override
     */
    unload(): void;
}

/**
 * Snapshot helper
 *
 * Takes a snapshot of the {@link ResponsiveCanvas} backing
 * a {@link CanvasRenderer}. In general, ResponsiveCanvas
 * is multi-layer therefore two capturing modes are available:
 * capture all layers separately or merge them into one image.
 * When you call snapshot() method Snapshoter waits for the
 * next render update and makes a copy of all canvas layers.
 * There're several modes for the resolution of the snapshot:
 * "video" - snapshot has the same size as the video stream,
 * "max"/"min" - in maximum/minimum size among canvas layers.
 */
declare class Snapshoter {
    protected renderer: CanvasRenderer;
    protected mirror: boolean;
    protected sizeMode: "video" | "max" | "min";
    protected sizeMax?: number | undefined;
    /**
     * Constructor
     *
     * @param renderer - Renderer to take snapshot of
     * @param mirror - Mirror captured images
     * @param sizeMode - Video size mode
     * @param sizeMax - Maximum video size
     */
    constructor(renderer: CanvasRenderer, mirror?: boolean, sizeMode?: "video" | "max" | "min", sizeMax?: number | undefined);
    /**
     * Take snapshot of the renderer
     *
     * Enqueues capture after the next renderer update.
     * All canvas layers are merged into one final image.
     *
     * @returns Promise resolved to the merged captured image
     */
    snapshot(): Promise<ImageData | undefined>;
    /**
     * Take snapshot of the renderer
     *
     * Enqueues capture after the next renderer update.
     * All canvas layers are returned as separate images.
     *
     * @returns Promise resolved to the array of captured images
     */
    snapshotLayers(): Promise<(ImageData | undefined)[]>;
}

/**
 * Recorder helper
 *
 * Records a video of the {@link ResponsiveCanvas} backing a
 * {@link CanvasRenderer}. ResponsiveCanvas is multi-layer.
 * Every rendering update Recorder merges all snapshots onto
 * the recording canvas. Then canvas'es content is grabbed and
 * encoded by MediaRecorder. Encoded video chunks are cached to
 * later be merged into one blob containing the final video file.
 * There're several modes for resolution of the recorded video:
 * "video" - records in the same resolution as the video stream,
 * "max"/"min" - in maximum/minimum size among layers of canvas.
 * Additionally one can limit resolution and bitrate of a video.
 */
declare class Recorder {
    protected renderer: CanvasRenderer;
    protected type: string;
    protected mirror: boolean;
    protected sizeMode: "video" | "max" | "min";
    protected sizeMax?: number | undefined;
    protected bitRate?: number | undefined;
    /** Canvas to capture video */
    protected canvas: HTMLCanvasElement;
    /** Drawing context of capturing canvas */
    protected context: CanvasRenderingContext2D | null;
    /** Video stream */
    protected stream?: MediaStream;
    /** Video recorder */
    protected recorder?: MediaRecorder;
    /** Record chunks */
    protected records: Blob[];
    /**
     * Constructor
     *
     * @param renderer - Renderer to record video from
     * @param type - Media/video type
     * @param mirror - Mirror captured images
     * @param sizeMode - Video size mode
     * @param sizeMax - Maximum video size
     * @param bitRate - Video stream bit rate
     */
    constructor(renderer: CanvasRenderer, type?: string, mirror?: boolean, sizeMode?: "video" | "max" | "min", sizeMax?: number | undefined, bitRate?: number | undefined);
    /**
     * Start video recording
     *
     * On every render update Recorder draws all layers onto
     * recording canvas. Chunks of encoded media stream are
     * cached to later be merged into one blob on stop().
     *
     * @returns True if recording started, False otherwise
     */
    start(): boolean;
    /**
     * Stop video recording
     *
     * Renderer stops recording and then merges encoded
     * video stream chunks into one blob returned to user.
     *
     * @returns Promise resolved to Blob containing encoded video
     */
    stop(): Promise<Blob | undefined>;
    /**
     * Dispose recorder object
     *
     * Releases resources and instances allocated by recorder.
     * Recorder object cannot be used after calling dispose().
     * One needs to stop recording before disposing the object.
     */
    dispose(): void;
    /**
     * Renderer update callback
     *
     * On every update Recorder draws all layers onto recording
     * canvas and requests video track to capture a new frame.
     */
    protected frame: () => void;
}

/**
 * Streamer helper
 *
 * Streams video of the {@link ResponsiveCanvas} backing a
 * {@link CanvasRenderer}. ResponsiveCanvas is multi-layer,
 * Every rendering update Streamer merges all snapshots onto
 * the recording canvas. MediaStream instance is created for
 * this canvas and provides access the generated video stream.
 * There're several modes for resolution of the streamed video:
 * "video" - streams in the same resolution as the video stream,
 * "max"/"min" - in maximum/minimum size among layers of canvas.
 * Additionally one can limit maximum resolution of the stream.
 */
declare class Streamer {
    protected renderer: CanvasRenderer;
    protected mirror: boolean;
    protected sizeMode: "video" | "max" | "min";
    protected sizeMax?: number | undefined;
    /** Canvas to capture video */
    protected canvas: HTMLCanvasElement;
    /** Drawing context of capturing canvas */
    protected context: CanvasRenderingContext2D | null;
    /** Output video stream */
    protected stream?: MediaStream;
    /**
     * Constructor
     *
     * @param renderer - Renderer to record video from
     * @param mirror - Mirror captured images
     * @param sizeMode - Video size mode
     * @param sizeMax - Maximum video size
     */
    constructor(renderer: CanvasRenderer, mirror?: boolean, sizeMode?: "video" | "max" | "min", sizeMax?: number | undefined);
    /**
     * Start video streaming
     *
     * On every Renderer update Streamer draws all layers onto
     * recording canvas. The canvas is a source of video stream.
     *
     * @returns True if recording started, False otherwise
     */
    start(): boolean;
    /**
     * Pause video streaming
     *
     * Stops drawing Renderer layers onto recording canvas.
     */
    pause(): void;
    /**
     * Media stream of generated video
     *
     * @returns Media stream
     */
    mediaStream(): MediaStream | undefined;
    /**
     * Render
     *
     * Every Renderer update Streamer merges snapshots of
     * multilayer ResponsiveCanvas onto recording canvas.
     */
    protected render: () => void;
}

export { AsyncEngine, CanvasMode, CanvasParams, CanvasRenderer, Engine, EngineParams, EventEmitterT, ImageBuffer, ImageBytes, ImageInput, ImageTexture, Plugin, PluginRenderer, ProcParams, Processor, Recorder, Renderer, ResponsiveCanvas, ScenePlugin, SceneRenderer, ShaderPlugin, ShaderProgram, ShaderRenderer, Size, Snapshoter, Streamer, UniformType, VideoCapture, VideoParams, VideoPlugin, VideoRenderer, VideoSource, VideoSourceParams };
