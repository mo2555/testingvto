## Renderer

{@link @geenee/armature!Renderer} is the core visualization
and logical part of any application. It's attached to the
{@link @geenee/armature!Engine}. Basically, renders define
two methods {@link @geenee/armature!Renderer#load | load()}
and {@link @geenee/armature!Renderer#update | update()}. The
first one is used to initialize assets and prepare the scene
(lightning, environment map). The second is used to update
the scene according to results of video processing. This's
where all the logic happens. Renderers can be extended with
plugins. Plugins do simple rendering task, for example add
object to that follows the head or render avatar overlay.

This package provides set of ready-made renderers and plugins
to simplify development of applications. They can be used as
both atomic building blocks or you can use them as starting
points, inherit and override / extend class functionality.
By extending {@link @geenee/armature!Renderer#load} and
{@link @geenee/armature!Renderer#update} of a Renderer or
Plugin's {@link @geenee/armature!ScenePlugin#load} and
{@link @geenee/armature!ScenePlugin#update} you can add
any custom logic, interactions, animations, post-processing,
effects, gesture recognition, etc.

Module utilizes babylon.js rendering engine for visualization.
Three.js renderers and plugins can be found in
{@link @geenee/bodyrenderers-three!} package.

## Basics

Set of abstract classes that specialize generic renderers
and plugins for {@link @geenee/bodyprocessors!PoseProcessor}
and {@link @geenee/bodyprocessors!FaceProcessor}.
These classes are used as base parents to simplify API,
they do not implement any logic or visualization.

* {@link PoseRenderer} - Abstract renderer for {@link @geenee/bodyprocessors!PoseProcessor}
* {@link PosePlugin} - Abstract plugin for {@link PoseRenderer}
* {@link FaceRenderer} - Abstract renderer for {@link @geenee/bodyprocessors!FaceProcessor}
* {@link FacePlugin} - Abstract plugin for {@link FaceRenderer}
* {@link BabylonRenderer} - Generic babylon.js renderer
* {@link BabylonPlugin} - Generic plugin for {@link BabylonRenderer}

## Pose Tracking

### {@link PoseAlignPlugin}
Universal plugin aligning node's rig and pose estimated by
{@link @geenee/bodyprocessors!PoseProcessor}. It's a base of
try-on, twin, and other plugins. You can use this class as a
starting point and customize alignment method or add features.
Basically, PoseAlignPlugin evaluates positions and rotations
of armature bones based on 3D pose keypoints, then applies
these transforms to bones following the armature hierarchy.
Plugin supports model rigs compatible with Mixamo armature,
e.g. any model from Mixamo library or Ready Player Me avatars.
This is common standard of armature/skeleton for human-like /
anthropomorphic models supported by many game/render engines.
The scene node must contain an armature among its children.
Armature's bones must follow Mixamo / RPM naming convention.
Models rigged and skinned manually or using Mixamo tool can
variate depending on anthropomorphous topology of the model.
For example animated characters can have disproportional body
parts like a much bigger head or longer arms. In such cases
PoseAlignPlugin can apply number of fine-tuning adjustments
to basic alignment improving model fitting or making it look
more natural. {@link PoseTuneParams} explains tuning options.
As an example turning off adjustment of spine curvature gives
better results in virtual garment try-on experiences, while for
full-body avatar overlaying it can provide more natural look.
Depending on the use case and model's topology you can try to
tune different options and see what works better in practice.
By default the plugin is fine-tuned for RPM avatars so you
can simply replace person with the avatar model in the scene.

### {@link PoseOutfitPlugin}
PoseOutfitPlugin is extension of {@link PoseAlignPlugin}
that allows to specify body meshes of the avatar's node
as occluders and optionally hide some child meshes (parts).
It's a good starting point for virtual try-on applications.
{@link OutfitParams} defines available options of outfit.
You can download any Ready Player Me avatar which outfit
similar to final result, edit its outfit, re-skin model if
necessary. Then simply use this plugin to build try-on app.
Armature bones must follow Mixamo / RPM naming convention.

### {@link PoseTwinPlugin}
{@link PoseAlignPlugin} extension for digital twins mirroring
the pose and residing beside a user. When rendering a twin we
do not translate bones to align with keypoint coordinates and
only preserve relative rotations. After projecting the detected
pose onto a twin, twin's scene node can be further transformed
relative to the initial position - centers of hips are the same.

### Pose Tracking Example
* Download pose tracking [example](/examples/pose-babylon.zip) for babylon.js
* Get access tokens on your [account page](https://builder.geenee.ar/sdk).
* Replace placeholder in `.npmrc` file with your personal NPM token.
* Run `npm install` to install all dependency packages.
* In `src/index.ts` set your SDK access tokens (replace stubs).
* Run `npm run start` or `npm run start:https`.
* Open `http(s)://localhost:3000` url in a browser.
* That's it, you first pose tracking AR application is ready.

### Preparing Models
Guides on preparing models for pose tracking:
* [General](/guides/models/body/body.md)
* [Automatic weights](/guides/models/body/automatic.md)
* [Weight transfer](/guides/models/body/transfer.md)
* [Mixamo](/guides/models/body/mixamo.md)

## Face Tracking

### {@link HeadTrackPlugin}
Plugin attaches provided scene node to the head.
Pose of the node (translation + rotation + scale)
continuously updates according to pose estimation
by {@link @geenee/bodyprocessors!FaceProcessor}.
Children nodes inherently include this transform.
The node can be seen as a virtual placeholder for
real object. It's recommended to attach top-level
nodes that don't include transforms relative to
parent, otherwise head transform that is a pose
in the world frame will be applied on top of them
(will be treated as relative instead of absolute).
Optionally anisotropic fine-tuning of the scale can
be applied. In this case model will additionally
adapt to shape of the face. If face isn't detected
by FaceProcessor plugin recursively hides the node.

Download reference face model: [face.glb](/models/face.glb).

Simplified implementation:
```typescript
async update(result: FaceResult, stream: HTMLCanvasElement) {
    if (!this.loaded)
        return;
    const { transform } = result;
    if (!transform) {
        this.node.setEnabled(false);
        return super.update(result, stream);
    }
    // Mesh transformation
    const translation = babylon.Vector3.FromArray(transform.translation)
    const uniformScale = new babylon.Vector3().setAll(transform.scale);
    const shapeScale = babylon.Vector3.FromArray(
        transform.shapeScale).scale(transform.scale)
    const rotation = babylon.Quaternion.FromArray(transform.rotation);
    // Align node with the face
    this.node.setEnabled(true);
    this.node.rotationQuaternion = rotation;
    this.node.position = translation;
    this.node.scaling = this.shapeScale ? shapeScale : uniformScale;
    // Render
    return super.update(result, stream);
}
```

### {@link FaceTrackPlugin}
Plugin attaches provided node to the face point.
Pose of the node (translation + rotation + scale)
continuously updates according to pose estimation
by {@link @geenee/bodyprocessors!FaceProcessor}.
Children nodes inherently include this transform.
The node can be seen as a virtual placeholder for
real object. It's recommended to attach top-level
nodes that don't include transforms relative to
parent, otherwise head transform that is a pose
in the world frame will be applied on top of them
(will be treated as relative instead of absolute).
Optionally anisotropic fine-tuning of the scale can
be applied. In this case model will additionally
adapt to shape of the face. If face isn't detected
by FaceProcessor plugin recursively hides the node.

Download reference face model: [face.glb](/models/face.glb).

Simplified implementation:
```typescript
async update(result: FaceResult, stream: HTMLCanvasElement) {
    if (!this.loaded)
        return;
    const { transform, metric } = result;
    if (!transform || !metric) {
        this.node.setEnabled(false);
        return super.update(result, stream);
    }
    // Mesh transformation
    const translation = babylon.Vector3.FromArray(metric[this.facePoint])
    const uniformScale = new babylon.Vector3().setAll(transform.scale);
    const shapeScale = babylon.Vector3.FromArray(
        transform.shapeScale).scale(transform.scale)
    const rotation = babylon.Quaternion.FromArray(transform.rotation);
    // Align node with the face point
    this.node.setEnabled(true);
    this.node.rotationQuaternion = rotation;
    this.node.position = translation;
    this.node.scaling = this.shapeScale ? shapeScale : uniformScale;
    // Render
    return super.update(result, stream);
}
```

### {@link FaceMaskPlugin}
Adds a Mesh object to the scene that reflects detected face mesh.
FaceMaskPlugin creates Mesh and defines indices, uvs and normals
of vertices in {@link FaceMaskPlugin#load | load()}, while vertex
positions are updated in {@link FaceMaskPlugin#update | update()}
according to current face tracking estimations. The plugin uses
StandardMaterial with a diffuse texture provided as image url.

Download face UV map: [faceuv.png](/models/faceuv.png)

Simplified implementation:
```typescript
async load(scene?: babylon.Scene) {
    if (this.loaded || !scene)
        return;
    // Mask
    const material = new babylon.StandardMaterial(
        "MaskMaterial", scene);
    material.diffuseTexture = new babylon.Texture(
        this.url, scene, undefined, false);
    material.diffuseTexture.hasAlpha = true;
    material.useAlphaFromDiffuseTexture = true;
    material.sideOrientation =
        babylon.Material.CounterClockWiseSideOrientation;
    material.backFaceCulling = true;
    var data = new babylon.VertexData();
    data.positions = meshReference.flat();
    data.indices = meshTriangles.flat();
    data.uvs = meshUV.flat();
    data.normals = [];
    babylon.VertexData.ComputeNormals(
        data.positions, data.indices, data.normals);
    this.mask = new babylon.Mesh("Mask", scene);
    this.mask.material = material;
    this.mask.renderingGroupId = 1;
    data.applyToMesh(this.mask, true);
    return super.load();
}

async update(result: FaceResult, stream: HTMLCanvasElement) {
    if (!this.loaded)
        return;
    if (!this.mask)
        return super.update(result, stream);
    const { metric } = result;
    if (!metric) {
        this.mask.setEnabled(false);
        return super.update(result, stream);
    }
    // Update mesh coordinates
    this.mask.setEnabled(true);
    this.mask.updateVerticesData(
        babylon.VertexBuffer.PositionKind, metric.flat());
    // Render
    return super.update(result, stream);
}
```

### Face Tracking Example
* Download face tracking [example](/examples/face-babylon.zip) for babylon.js
* Get access tokens on your [account page](https://builder.geenee.ar/sdk).
* Replace placeholder in `.npmrc` file with your personal NPM token.
* Run `npm install` to install all dependency packages.
* In `src/index.ts` set your SDK access tokens (replace stubs).
* Run `npm run start` or `npm run start:https`.
* Open `http(s)://localhost:3000` url in a browser.
* That's it, you first face tracking AR application is ready.

### Preparing Models
Guides on preparing models for face tracking:
* [General](/guides/models/face/face.md)
* [Head tracking](/guides/models/face/head.md)
* [Face tracking](/guides/models/face/point.md)

## Occluders

### {@link OccluderPlugin}
Plugin making provided node an occluder. Usually
node is a base mesh (average approximation) of a
body representing its real counterpart in a scene.
Occluders are not rendered by themselves but still
participate in occlusion queries. This is achieved
by setting `disableColorWrite=true` to materials of
node's meshes. This flag tells rendering engine to
not write to color buffer but still write to depth
buffer. Then meshes are effectively not rendered
(fragment color write is skipped) and only occlude
all other meshes of the scene (during depth test).

Simplified implementation:
```typescript
async load(scene?: babylon.Scene) {
    if (this.loaded || !scene)
        return;
    // Occluder material
    if (this.node instanceof babylon.AbstractMesh) {
        if (this.node.material) {
            this.node.material.disableColorWrite = true;
            this.node.material.needDepthPrePass = true;
        }
        this.node.renderingGroupId = 0;
    }
    this.node.getChildMeshes(false).forEach((mesh) => {
        if (mesh.material) {
            mesh.material.disableColorWrite = true;
            mesh.material.needDepthPrePass = true;
        }
        mesh.renderingGroupId = 0;
    });
    return super.load(scene);
}
```
